# Artificial Intelligence and Society

This repository documents my (totally incomplete) exploration of the
philosophy of ethics and society with a particular interest in the role
and future of artificial intelience.

So far most of the notes and resources are based on what I learned as a
tutor for
[*COMP90087 The Ethics of Artificial Intelligence*](https://handbook.unimelb.edu.au/2021/subjects/comp90087)
at the University of Melbourne, a subject discussing current issues in
AI and society, framed through introductory level moral philosophy.

In the future I will add further contents based on my own reading, research,
and thought,
and further coursework (such as the EA Cambridge
[Fellowship on AGI Fundamentals](https://www.eacambridge.org/agi-safety-fundamentals).
I will also extend the topics beyond near-term AI issues to the long-term
future safety and impact of AI.

If we're lucky this repository might eventually become a website, or even a
book (don't hold your breath).


#### Contents

This repository is a work-in-progress. Please feel free to raise an issue if
you want some more of that progress or have any other questions.

* [COMP90087 The Ethics of Artificial Intelligence](comp90087/):
  Some information about the subject, the staff, and the official syllabus;
  plus *readings!*
* My notes on ethics and AI (below):
  My list of key topics, lessons, and readings, and some of my remaining
  questions.

---

## My notes on ethics and AI

On reflection, I feel like the COMP90087 subject could have been organised
into a more coherent narrative; and many of our discussions, though grounded
in lots of realistic case studies, could have delved deeper into the
underlying moral philosophy.

In this section I will give a list of key topics and ideas from
the subject, with more emphasis on fundamentals and less on case studies and
examples, as suits my particualr interest. I will also note down important
lessons I have learned about the topics and the process of studying and
teaching them. Finally, I will refer to key readings (not all of which I have
yet gotten around to reading) and, importantly, key open questions (from my
perspective) left unanswered by COMP90087 where I would like to investigate
further some day.

### My rough syllabus

Here's a rough high-level syllabus/narrative if I were to try (however
underqualified I am) to put together a subject like this myself:

* The history of AI
* Philosophy of mind
* Moral philosophy / ethics
  * *Normative v. descriptive claims and ethical arguments*
  * Traditional ethical frameworks
    * Consequentialism and utilitarianism
    * Deontology and Kant's Categorical Imperative
    * Virtue Ethics
    * Ethics of Care
    * Principlism
    * *There are more!* Preferences? Theology? Economic growth? Etc.
  * Practical ethical guidelines
* Political philosophy---the law, policy, politics, governance (think
  about this structure more)
* Ethical principles
  * Trust
  * Fairness / justice
  * Accountability
  * Privacy and data
  * Transparency
  * Explainability
  * Accessibility
  * Equity
  * Rights
  * *Autonomy*
  * *Personhood*
* Technologies and issues
  * Automation
  * *Surveillance*
  * Facial recognition
  * Deepfakes
  * Automatic justice
* *Automated ethics?*
  * *Superintelligence*
  * *Reward learning and Beneficial AI*
* More?

---

> ### Lesson number 1: AI problems are often just people problems
> 
> At many points during lectures, discussions, and readings throughout the
> semester, I realised that if we took the AI out of the discussion (for
> example by mentally replacing the AI system with a human or a collection of
> humans), we would still have the same ethical issue to resolve. 
> Not all of the issues we discussed were inherently related to the AI!
> (Though the AI or technology sometimes magnified and/or distorted the
> issues.)
> 
> I think this observation can help us to resolve ethical issues:
> In the case where we can separate the "AI" part from the case study without
> fundamentally changing the ethical issue, we effectively reduce the problem
> a human problem, which is potentially easier to solve
> (and we can leverage the millenia of thought on moral and political
> philosophy that has come before the invention of AI systems to help us do
> so).
> 
> On the other hand, if replacing the AI with a human makes the problem go
> away, then it would seem that the human has some feature(s) which the AI is
> lacking and that this lack is causing the issue. Then identifying these
> missing features (the way humans solve the ethical problem) gives us a
> template by which we can solve the AI-ethical problem.
> 
> This explanation could benefit from some examples. Since this has been such
> a common theme, I'll attempt to reference it in my discussion throughout the
> rest of these notes.

---

> ### Lesson number 2: Interdisciplinary communication is hard (duh)
> 
> In conversations with the teaching team, I often had much more trouble
> comprehending and being comprehended than I am accustomed to. It turns out
> people from different disciplines can sometimes use the same words to mean
> completely different things. Okay, maybe I should have seen this coming, but
> it's one thing to say and another thing to experience. Anyway, since I am
> interested in pursuing multidisciplinary work in my research career, I will
> have to talk and collaborate with people from all kinds of disciplinary
> backgrounds. I had better remember this lesson and be aware of the language
> barriers when I am speaking, listening, reading, and writing!
