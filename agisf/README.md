# AGI Safety Fundamentals

A fellowship / faciliated reading group on foundational ideas in the study
of the safety of generally intelligent AI systems.
The fellowship is run by [EA Cambridge](https://www.eacambridge.org/).
The curriculum is curated by [Richard Ngo](http://thinkingcomplete.blogspot.com/p/about.html).

I was honoured to be accepted into the second iteration of the program,
starting in July 2021. The number of interested students was apparently
quite high, and so was the number of accepted students---I joined cohort 27.

More information about the fellowship, including materials, is available
[online](https://www.eacambridge.org/agi-safety-fundamentals).
This page will be my own version of the resource list, along with
any additional resources I think are relevant, and perhaps some notes
from my participation in the program.

## Syllabus

Each week consists of several readings on a common topic, and a facilitated
discussion session within cohorts. Below is a list of topics by week:

0. Introduction to machine learning
1. AGI and superintelligence
2. Goals and misalignment
3. Threat models and types of solutions
4. Learning from humans
5. Decomposing tasks for outer alignment
6. Other paradigms for safety work
7. AGI safety in context

The course will conclude with an optional, multi-week personal project
(e.g. blog post, literature rview, explanation, brainstorm, etc.)
to be discussed in a final, faciliated '8th' week.

The course does not have a single 'textbook', but along with acceptance into
the program came a free book from the three 'AI Safety' texts:

* Nick Bostrom, *Superintelligence*, 2015
* Stuart Russell, *Human Compatible*, 2019
* Brian Christian, *The Alignment Problem*, 2020

I requested the latter since I already had copies of the former two.


## Week 0: Introduction to machine learning

[Reading list](https://www.eacambridge.org/agi-week-0)

## Week 1: AGI and superintelligence

> What do we mean by artificial general intelligence, and how might we
> achieve it?

[Reading list](https://www.eacambridge.org/agi-week-1).

Core readings:

* Luke Muehlhauser, "What is AGI?",
  MIRI [blog post](https://intelligence.org/2013/08/11/what-is-agi/), 2013.
* Richard Ngo, "Superintelligence", *AGI Safety from First Principles*
  [sequence](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ),
  2020.
* Paul Christiano, "Three Impacts of Machine Intelligence",
  [rationalaltruist.com](https://rationalaltruist.com/2014/08/23/three-impacts-of-machine-intelligence/),
  2014.
* Richard Sutton, "The Bitter Lesson",
  [incompleteideas.net](http://www.incompleteideas.net/IncIdeas/BitterLesson.html),
  2019.

Extra reading:

* Rohin Shah, "Reframing Superintelligence",
  [alignmentforum.org](https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as),
  2019.
  * Summary of: Eric Drexler, "Reframing Superintelligence",
    FHI Technical Report, 2019.
* Paul Christiano, "Prosaic AI alignment",
  [ai-alignment.com](https://ai-alignment.com/prosaic-ai-control-b959644d79c2),
  2016.
* OpenAI, "AI and Compute",
  [OpenAI.com](https://openai.com/blog/ai-and-compute/), 2018.
* OpenAI, "AI and Efficiency",
  [OpenAI.com](https://openai.com/blog/ai-and-efficiency/), 2020.
  * Corresponding paper: Hernandez and Brown, "Measuring the Algorithmic
    Efficiency of Neural Networks", [arXiv](https://arxiv.org/abs/2005.04305),
    2020.
* Griffiths, "Understanding Human Intelligence through Human Limitations",
  in press, 2020.
* Eliezer Yudkowsky, "The Power of Intelligence",
  MIRI [blog](https://intelligence.org/2007/07/10/the-power-of-intelligence/),
  2007.

Exercises:

* A crucial feature of AGI is that it will possess cognitive skills which
  are useful across a range of tasks, rather than just the tasks it was
  trained to perform. Which cognitive skills do humans possess that are
  useful both in our modern environment, and in the ancestral environment
  in which we evolved?

* Optional: what are the most likely ways that the hypothesis that we will
  build AGIs which have transformative impacts on the world might be false?

