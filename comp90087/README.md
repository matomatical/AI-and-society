# COMP90087 The Ethics of Artificial Intelligence

Semester 1, 2021, was the inaugural semester of the subject
[*COMP90087 The Ethics of Artificial Intelligence*](https://handbook.unimelb.edu.au/2021/subjects/comp90087)
at the University of Melbourne. This subject aims to introduce (mostly)
technical postgraduate students to topics in AI history, moral philosophy,
digital ethics, and digital law,
with an emphasis on the particular ethical issues raised by applications of AI
technologies.
(I guess) the hope is that this education will help these students think more
carefuly and responsibly about the ethical implications of their future work
as technologists.

That sounded like a great plan to me, and I had always wanted to sit down and
learn some moral philosophy for my own self-improvement, so I was excited to
apply to be a part of the teaching team, and I was honoured to be accepted as
a COMP90087 tutor!

Overall, the experience of teaching this subject left me with many thoughts
and questions unresolved, and so one day I hope to find time to take a deeper
dive into the underlying moral philosophy of the issues we have discussed.


## Teaching team

The subject was developed by an interdisciplinary team of researchers, mostly
from the University's
[*Center for Artificial Intelligence and Digital Ethics (CAIDE)*](https://law.unimelb.edu.au/centres/caide/):

* [Professor Tim Miller](https://people.eng.unimelb.edu.au/tmiller/),
* [Dr Simon Coghlan](https://findanexpert.unimelb.edu.au/profile/787891-simon-coghlan),
* [Dr Marc Cheong](https://findanexpert.unimelb.edu.au/profile/862627-marc-cheong),
* [Professor Jeannie Patterson](https://law.unimelb.edu.au/about/staff/jeannie-paterson),
* [Dr Kobi Leins](https://findanexpert.unimelb.edu.au/profile/626407-kobi-leins), and
* Dr Michael Wildenauer,

with Michael, Kate Ferris, and *yours truly* delivering the weekly tutorials,
and Gabby Bush helping it all come together.


## Subject syllabus

Each week was broadly focussed around a single topic. There was a prerecorded
lecture playlist from the subject developers along with a live seminar/chat,
some readings (see below). Oh, and a tutorial, that's where I came in!
Below is a list of topics by week:

1. Trust, machines, and digital ethics (Tim)
2. The History of Artificial Intelligence (Tim)
3. Philosophy and ethics (Simon)
4. Fairness and accountability (Simon)
5. Data governance (Marc)
6. Accessibility and equity (Marc)
7. Transparency---decisions & processes (Marc)
8. Explainability (Tim)
9. Policy, politics, and AI (Jeannie and Michael)
10. Frameworks and implementation (Michael)
11. AI and human rights (Michael)
12. Bringing it together (Simon)



## Subject readings

Below is the list of readings from the subject, oranised by week.

The reading descriptions are from the Canvas page (written by the subject
coordinators, **not me**).

I don't totally support the choice of topics or readings.
In general I think it's valuable to start with a higher-level overview
from e.g. the Stanford Encyclopedia of Philosophy and associated bibliography.


### Module 00: Introduction

This module has one optional reading:

1. A. Tsamados et al., "The Ethics of Algorithms: Key Problems and Solutions", 2020.



### Module 01: Trust, machines, and digital ethics

This module has two readings:

1. Jacovi, A., Marasović, A., Miller, T., Goldberg, Y.,
   "Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and
   Goals of Human Trust in AI," FAccT, 2021.

   This paper (co-authored by Tim) presents a model of trust in AI that is
   inspired by the notion of interpersonal trust (i.e., trust between
   people), proposing the notion of contractual trust in AI, where a
   contract is a (legal or a social) expectation of what an AI should be
   able to do.
   When a model fails to uphold a contract that a person expects, that
   person may consider that the model is no longer trustworthy, and will
   lose trust in the model. The relation to digital ethics is that
   assessing whether a particular application is 'ethical' is largely
   around whether we assess that the application upholds the contracts that
   we think are important.
   For example, does the application exhibit unreasonable bias in decision
   marking, is the application accessible to the right people, or does the
   application properly preserve privacy?

2. Parasuraman, R., Riley, V.,
   "Humans and Automation: Use, Misuse, Disuse, Abuse,"
   Human Factors 39, 1997.

   Written in 1997, this paper discusses the consequences of using
   automation (machines, software, etc.) when trust is incorrectly
   calibrated; that is, a person or persons have either too little or too
   much trust in a system. It introduces the important concepts of use,
   misuse, disuse, and abuse. Despite being well over 20 years old, this
   paper is still a key paper in the area of trust of machines;
   importantly, it foreshadowed a lot of research on the under-reliance
   and over-reliance in software systems. This paper gives us an framework
   for thinking about how and what can go wrong when people have incorrectly
   calibrated levels of trust in a system, which has implications for
   digital ethics.



### Module 02: The history of artificial intelligence

Two main readings:

1. The wikpedia entry [History of Artificial Intelligence](https://en.wikipedia.org/wiki/History_of_artificial_intelligence).
    
    While Wikipedia articles are not typically suitable academic references, to quote Marc: "history is dynamic/non-definitive anyway", and this is the most complete publicly accessible document that we could find.  There are several nice articles on the history of AI, but these are typically not up to date, or in some cases, quite inaccurate about even commonly-accepted things. The Wikipedia article does a nice job of describing the different eras of artificial intelligence and linking in the important key players of this history, which is important to our discussions.

2. Marc Cheong, Kobi Leins, Simon Coghlan.
  "Computer Science Communities: Who is Speaking, and Who is Listening to
  the Women? Using an Ethics of Care to Promote Diverse Voices"
  FAccT 2021.

   This excellent article by three of the subject coordinators (Marc, Kobi, and Simon) looks at the quite recent history of computer science and studies the (lack of) gender diversity in the field, highlighting some key issues that this creates. It proposes the applications of the Ethics of Care to help promote a diversity of different voices in the field. But the Ethics of Care is not just about gender diversity. It can be used to promote voices from any marginalised community: gender, racial background, cultural background, sexual preference, disabilities, etc.

Optional readings:

1. Michael Wooldridge, *The Road to Conscious Machines: The Story of AI,* 2020.

   Chapters 1-5 of this excellent book cover the history of AI, including a deep look into the successes and failures of different eras. The remaining chapters look forward as to how we can get to machines that can reason like people. Spoiler alert: the author is very skeptical of this happening any time soon!

2. Virginia Eubanks, *Automating inequality: How high-tech tools profile,
   police, and punish the poor.*, 2018.

   This groundbreaking and carefully researched book looks more generally at technology, and how it is used to reinforce stereotypes that we have about marginalised groups. Through her own groundwork, conducting many interviews with people whose lives have been severely impacted by technology, Eubanks gives us an insight into the negative impacts from seemingly "harmless" technology.



### Module 03: Philosophy and ethics

Three main readings:

1. Hagendorff, Thilo. "The ethics of AI ethics: An evaluation of guidelines,"
   Minds and Machines 2020.

   This is a scholarly article that gives an up-to-date overview of the field of AI ethics. In this article, the author explains - and critiques - major ethics guidelines for AI and the forces that affect the trajectory and development of Artificial Intelligence. The author suggests that certain ethical approaches and principles have been downplayed in AI ethics so far. He argues for recognising a wider range of ethical approaches or frameworks in this field. This article will help to get us thinking about the role that ethics and philosophy should play in understanding AI.


2. "Facial recognition: top 7 trends,", 2021,
   [Thales article](https://www.thalesgroup.com/en/markets/digital-identity-and-security/government/biometrics/facial-recognition)
  
   Gives an overview of face recognition and emotion recognition from a company's point of view. This gives us a bit of a taste of a possible future that might be just around the corner!


3. "Wrongfully Accused by an Algorithm," 2020,
   [New York Times article](https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html)

   Describes a case involving facial recognition that caused an outcry. Although it is about a single case of mis-identification, the story conveys the very real human effects that AI gone wrong can have. When you read it, ask yourself whether the story about this misidentified individual in the US has wider implications for the use of facial recognition and computer vision technology.   

Students who are looking for additional (optional) reading might consult:

1. Vincent Müller, "Ethics of artificial intellience and robotics," 2020,
  [Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/ethics-ai/)

   This article gives a wide-ranging overview of digital ethics, including identifying key contemporary debates in AI ethics and also describing related fields like robot ethics and machine ethics. The issue of the Singularity and Superintelligence is also explained.

2. James Fieser, "Ethics",
   [Internet Encyclopedia of Philosophy](https://iep.utm.edu/ethics/)

   For further information about the ethical frameworks (or 'theories') that
   we talk about in the recorded lectures.
   It might be useful when you are writing your essays.



### Module 04: Fariness and accountability

Two main readings:

1. Pak-Hang Wong, "Democratising Algorithmic Fairness", 2019.

   This reading discusses questions of fairness, why perfect algorithmic fairness is usually unachievable, and how we might ensure that those responsible for AI are kept accountable. It proposes an accountability framework called Accountability For Reasonableness (AFR). Although this framework might be contested, it gives us insight into both the issues of accountability and fairness at the same time.

2. John Searle, "Is the Brain's Mind a Computer Program?" 1990.

   This famous reading presents an argument against the idea that AI systems could potentially think, understand, be conscious, or have a mind. The article delves into the philosophy of mind. This territory may be unfamiliar to you, but the reading is meant to give you a sense of some of the ideas concerning AI and consciousness.


Further (optional) reading:

1. Mulligan et al.
   "This Thing Called Fairness: Disciplinary Confusion Realizing a Value in
   Technology", 2019.

   For more on algorithms and fairness.

2. Cole, David, "The Chinese Room Argument", 2020,
   [The Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/archives/win2020/entries/chinese-room/). 
   
   For much more detail on John Searle's Chinese Room.



### Module 05: Data governance

Two readings:

1. Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton,
   Alex Hanna. 
   "Data and its (dis)contents: A survey of dataset development and use in
   machine learning research." 2020.

   This academic paper written by researchers from Google, Mozilla and the University of Washington discusses issues related to development and governance of machine learning datasets. Datasets used in research on image classfication to natural language processing can actually contain potential to cause harms. This ranges from underrepresentation of minoritised groups (e.g. human image datasets containing a disproportionate distribution of ethnicities), to entrenching dangerous biases (e.g. sexism and racism in natural language datasets and the resulting models based on these datasets). Moreover, there are issues surrounding what goes into these datasets - from copyright to the data subjects' ownership.

2. Carl Öhman, Nikita Aggarwal.
   "What if Facebook goes down? Ethical and legal considerations for the
   demise of big tech."
   Internet Policy Review 9(3), 1-21, 2020.

   Now this is a very interesting paper that addresses an issue that is rarely talked-about, but should not be discounted, as many of us have social media accounts! The authors discuss the various legal and ethical and societal implications if tech giants (such as Facebook) shut down, or merely discontinue or dispose of some of their products under their offering. Who does Facebook (and other tech giants) have an obligation to? Do they have a duty of care for not just its active users, but also e.g. deceased users, future users, as well as the interesting idea of ‘dependent communities’? i.e. "communities and industries that have developed around the Facebook platform and now (semi-)depend on its existence to flourish" (Öhman & Aggarwal, 2020).



### Module 06: Accessibility and equity

Two readings. They are interesting academic papers on the topic
(one academic piece and one executive research report), which sheds light
onto how the tools and techniques we use in data science and computing can
sometimes be inaccessible and inequitable.

1. Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster,
   Yu Zhong, Stephen Denuyl,
   "Social Biases in NLP Models as Barriers for Persons with
   Disabilities," 2020.

   This paper discusses the fact that "representations encoded in [Natural Language Processing / NLP] models often inadvertently perpetuate undesirable social biases from the data on which they are trained" (Hutchinson et al, 2020). Why? For starters, language models are trained on large text corpora (plural of corpuses) gathered from the real world, reflective of real-world human biases and prejudices about disabilities. If we're not aware of these issues, and these biases end up in an application deployed for, say, automated decision-making, "perpetuation of societal stereotypes or inequities, or harms to the dignity of individuals" (Hutchinson et al, 2020) may result.

2. Marc Cheong, Reeva Lederman, Aidan McLoughney, Sheilla Njoto,
   Leah Ruppanner, Tony Wirth,
   "Ethical Implications of AI Bias as a Result of Workforce Gender
   Imbalance," 2020.
   [pdf](https://about.unimelb.edu.au/__data/assets/pdf_file/0024/186252/NEW-RESEARCH-REPORT-Ethical-Implications-of-AI-Bias-as-a-Result-of-Workforce-Gender-Imbalance-UniMelb,-UniBank.pdf).

   This research report is a result of an interdisciplinary collaboration between University of Melbourne and UniBank in uncovering sources of bias --  both human and algorithmic -- to consider when deploying any form of automated system in recruitment/shortlisting of job candidates.


If you find this module interesting, there is another paper which
systematically analyses gender bias, and a bestselling book covering
gender bias in a variety of data-driven settings.

1. Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes,
   Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram
   Kenthapadi, Adam Tauman Kalai,
   "Bias in Bios: A Case Study of Semantic Representation Bias in a
   High-Stakes Setting."
   2019, FAccT.

   This paper is a discussion of how gender bias is ingrained in "occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives" (De-Arteaga et al, 2019), using the Common Crawl text corpus.

2. Caroline Criado Perez, *Invisible Women: Exposing Data Bias in a World
   Designed for Men.*, 2020.



### Module 07: Transparency: Decisions and processes

This module has two readings focusing on the transparency of the processes
and decision-making that goes behind (and are consequences of) algorithmic
systems and their research. Here they are:

1. Toby Walsh, "Experiments in Social Media", AI Magazine, 2019.

   This is an interesting piece which highlights the dangers how studies/experiments on social media can contribute to "challenges[,] as even small effects when multiplied by a large population can have a significant impact". Experimental "interventions increased turnout by about 340,000 additional votes ... around 0.5% of the total number of votes cast" (Walsh, 2019) in a Facebook experiment to encourage voting in the 2010 US Elections. The author highlights the issues of (non)transparency in AI research especially on such a large scale.

2. Cynthia Rudin,
   "Stop Explaining Black Box Machine Learning Models for High Stakes
   Decisions and Use Interpretable Models Instead",
   Nature Machine Intelligence, 2019.

   The second reading for this week focuses on the pervasiveness of "black box machine learning models" (Rudin, 2019) and how their implementation -- in, say, decision making for criminal justice -- can be mired by a "lack of transparency and accountability of predictive models ... [with] severe consequences" (Rudin, 2019). We need to look at the processes and the decision making aspects behind the development and deployments of these systems. From a computer scientist's lens, we should ask questions like 'should we even use it on people?' rather than 'can we optimise it?'

Optional reading:

1. "Facebook–Cambridge Analytica data scandal",
   [Wikipedia article](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal)

   If you find this module interesting, you might want to check out the brief
   history of the Cambridge Analytica controversy on wikipedia.



### Module 08: Explainability

This module has one compulsory reading:

1. Tim Miller,
   '"But why?" Understanding explainable artificial intelligence',
   XRDS 25, 2019.
    
   This is an article that I wrote for XRDS Crossroads, which is the ACM magazine for students. It is a very general overview that answers why explainability is important, gives some challenges in obtaining explainability in artificial intelligence and a very high level overview of techniques.

There are also two other resources for those who are interested:

1. Vaishak Belle and Ioannis Papantonis,
   "Principles and Practice of Explainable Machine Learning", 2020.

   This is an overview of explainability algorithms and research. It is a good primer those who are interested in some of the more technical details of explainable machine learning.

2. Cristoph Molnar, *Interpretable machine learning*
   [web book](https://christophm.github.io/interpretable-ml-book/)

   This is a brilliant e-book on interpretable machine learning that starts at the very foundations of interpretable models and discusses many different methods. It contains lots of useful examples and source code. Christoph adds new chapters to the book as he reads more.



### Module 09: Policy and politics

AI Policy, Regulation and Law:

1. Gilad Edelman, "Why Don't We Just Ban Targeted Advertising?", 2020,
   [Wired article](https://www.wired.com/story/why-dont-we-just-ban-targeted-advertising/)

   This article talks about the policy decisions that might be taken in regulating targeted advertising. It does this, effectively, by weighing up the costs and benefits of different approaches. Is targeted advertising harmful? Who does it benefit? and what would be the harm if it was banned? This is how governments make policy (although sometimes politics clouds the analysis). Once the optional policy choice has been identified, then Government enacts laws and other regulatory strategies to put that policy into operation. 

2. Jeannie Marie Paterson and Elise Bant, 
   "Privacy Erosion by Design: Why the Federal Court Should Throw the Book
   at Google Over Location Data Tracking", 2021, 
   [Conversation article](https://theconversation.com/privacy-erosion-by-design-why-the-federal-court-should-throw-the-book-at-google-over-location-data-tracking-159206)

   This short piece reflects on the recent Federal Court decision finding Google misled (some) Australian consumers over privacy options when using an android phone. It considers the amount of the fine that should be imposed to deter this kind of conduct. You will be asked to consider the issues of privacy by design in the tutorial (aka making it easy for consumers to understand default settings).

Politics:

1. Diakopoulos, N. and Johnson, D.,
   "Anticipating and addressing the ethical implications of deepfakes in
   the context of elections."
   New Media & Society, 2019.

   This paper is quite recent, examining some of the ethical issues that one type of AI-driven speech, that of deepfakes, can have on the electoral process. While written from a US-centric point of view, it is relevant in many, if not, most, countries that have elected officials. And as deepfakes get easier and cheaper to make, perhaps their use will become more common in elections of all sorts: unions, student bodies, branches of political parties and others.

   I think that it is interesting that elections are one part belief system, one part clash of ideas, and one part personal attraction of the candidate. This allows all sorts of methods of influencing of elections by AI, with deepfakes aiming somewhat at ideas but mostly just trying to make a candidate look bad.



### Module 10: Frameworks and implementation

This module has only one compulsory reading, but it has a lot of content:

1. Beard, M and Longstaff, S.
   *Principles For Good Technology*,
   The Ethics Centre.
   [download](https://ethics.org.au/ethical-by-design/#download-copy)

   This is a really interesting take on the idea of a framework or set of guidelines. It is written in a more accessible style, has lots of visuals, and provides explanations for not only the principles the authors espouse, but also justifications for including them.

   While you take a look a the reading, keep the following questions in mind:
   
   Q: What do the terms 'frameworks' and 'implementation' mean to you?
   
   Q: What are the advantages and disadvantages of providing guidance in a less abstract and more accessible fashion?
   
   Q: Are guidelines/frameworks/principles of any real use to AI developers? Would you use such guidance?

For those of you interested in what the implementation of AI and Big Data can mean for ordinary people, particularly those in more marginalized groups, there is an optional reading available:

1. Cathy O'Neill, *Weapons of Maths Destruction*, 2016.

   This book is fantastic, placing data collection front and centre, and showing all that we miss. How could we see help to avoid overlooking large portions of the population?  What frameworks, if any, would help? Are frameworks the right approach? If not, what are?
   
   Even if you don't have time to read it now, I would encourage you to download the chapter provided, to read later. It's really important that those involved in the creation of systems that affect others understand some of the burdens and harms that may devolve to those others, often long after the developers have moved on to another project.
   
   What did this chapter make you think? Who should decide how the tools we create are used?
   
   Q: What frameworks would you develop if you were in charge? How would you ensure that they are implemented?



### Module 11: AI and human rights

This module has three readings, but two of these are quite short. The third
read attempts to rebut the first two.

1. Raso, F.A., Hilligoss, H., Krishnamurthy, V., Bavitz, C. and Kim, L.,
   "Artificial intelligence & human rights: Opportunities & risks", 2018.

   Read pages 1-16, 32-36, 52-58

2. Hilligoss, H., Raso, F.A., Krishnamurthy, V. 
   "It's not enough for AI to be 'ethical'; it must also be 'rights
   respecting'.", 2018,
   [blog post](https://medium.com/berkman-klein-center/its-not-enough-for-ai-to-be-ethical-it-must-also-be-rights-respecting-b87f7e215b97)

3. Canca, C., "Why Ethics Cannot be Replaced by the Universal Declaration of
   Human Rights,"
   [Our World article](https://ourworld.unu.edu/en/why-ethics-cannot-be-replaced-by-the-universal-declaration-of-human-rights)



### Module 12: Bringing it together

Just one more reading:

1. Bietti, E.,
   "From ethics washing to ethics bashing: a view on tech ethics from within
   moral philosophy," FAccT 2020.
